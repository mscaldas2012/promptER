# AI Prompt Builder

This Streamlit application helps users refine their system prompts for AI assistants using either a local Ollama LLM or Azure OpenAI.

## Features

*   **Prompt Refinement:** Submit an initial system prompt and receive a more effective and well-structured version generated by a Large Language Model (LLM). The response is provided in a structured JSON format, including review comments, suggested improvements, and a revised prompt.
*   **Iterative Refinement:** After receiving a refined prompt, you can choose to "Refine Again" to load the revised prompt back into the editor for further modifications and resubmission.
*   **Playground:** Test your refined prompts in a chat-style "Playground" environment to see how the AI responds with the new system prompt.
*   **Provider Selection:** Choose between Ollama (local LLM) and Azure OpenAI (cloud-based LLM) as your AI provider.
*   **Model Selection:** Select from a list of available models for the chosen provider directly within the application.
*   **Configurable System Prompt:** The core system prompt for the prompt builder itself is externalized in `config.json`, allowing for easy modification and versioning.
*   **LLM Call Logging:** Every interaction with the LLM is logged in a structured JSON format (`llm_calls.log`), providing details such as timestamp, prompt ID, model used, tokens, latency, and outcome.
*   **Secure Credential Management:** Azure OpenAI API keys and endpoints are loaded securely from a `.env` file, keeping sensitive information out of the codebase.

## Setup

Follow these steps to get the AI Prompt Builder running on your local machine.

### Prerequisites

*   **Python 3.8+:** Ensure you have Python installed.
*   **Ollama (Optional, for local LLM):** If you plan to use Ollama, you need to have it installed and running. Download it from [ollama.ai](https://ollama.ai/).
*   **Ollama Models (Optional):** If using Ollama, download the required LLM models (e.g., `llama2`, `mistral`, `codellama`) using the Ollama CLI. For example:
    ```bash
    ollama pull llama2
    ollama pull mistral
    ollama pull codellama
    ```
*   **Azure OpenAI Account (Optional, for cloud LLM):** If you plan to use Azure OpenAI, you will need an active subscription, an Azure OpenAI resource, and a deployed model.

### Installation

1.  **Clone the repository (if applicable) or navigate to the project directory:**
    ```bash
    cd /Users/m/projects/work/od/promptER
    ```

2.  **Create a Python virtual environment:**
    ```bash
    python3 -m venv .venv
    ```

3.  **Activate the virtual environment:**
    *   On macOS/Linux:
        ```bash
        source .venv/bin/activate
        ```
    *   On Windows:
        ```bash
        .venv\Scripts\activate
        ```

4.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration

The application's behavior is configured via two main files: `providers.json` and `config.json`.

### `providers.json`

This file contains the configuration for the LLM providers.

```json
{
    "provider": "ollama",
    "ollama": {
        "model": "llama2",
        "models": [
            "llama2",
            "mistral",
            "codellama"
        ]
    },
    "azure_openai": {
        "models": [
            "gpt-4o",
            "gpt-35-turbo"
        ]
    }
}
```

*   `provider`: The default LLM provider to use (`ollama` or `azure_openai`).
*   `ollama.models`: A list of available Ollama models that the user can select from.
*   `azure_openai.models`: A list of available Azure OpenAI models (deployment names) that the user can select from.

### `config.json`

This file defines the model configuration for the prompt refinement feature.

```json
{
    "model_config": {
        "id": "sp-4f8c2e",
        "version": "0.3.0",
        "purpose": "To assist users in refining system prompts for AI assistants and provide feedback in a structured JSON format.",
        "owner": "Gemini",
        "date_created": "2025-10-23",
        "intended_model": "all",
        "tags": [
            "prompt-engineering",
            "ai-assistant",
            "refinement",
            "json"
        ],
        "notes": "This version of the system prompt enforces a JSON output format and uses a roles-based structure.",
        "roles": {
            "system": "You are an AI assistant skilled at reviewing and refining prompts for optimal performance...",
            "assistant": "The result must be a JSON object..."
        }
    }
}
```

*   `model_config`: An object containing metadata and the roles for the prompt refiner.
    *   `id`, `version`, `purpose`, etc.: Metadata about the model configuration.
    *   `roles`: An object where each key is a role name (e.g., "system", "assistant") and the value is the prompt for that role. This allows for dynamic construction of the LLM messages.

### `.env` file

If you are using Azure OpenAI, create a file named `.env` in the root directory of the project (the same directory as `app.py` and `config.json`). This file will store your sensitive API credentials.

**`.env.template`:**
```
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_DEPLOYMENT_NAME=
AZURE_OPENAI_API_VERSION=
```

**Example `.env` file (replace with your actual credentials):**
```
AZURE_OPENAI_API_KEY="your_azure_openai_api_key_here"
AZURE_OPENAI_ENDPOINT="https://your-resource-name.openai.azure.com/"
AZURE_OPENAI_DEPLOYMENT_NAME="your-deployment-name"
AZURE_OPENAI_API_VERSION="2025-01-01-preview"
```

**Important:** Do not commit your `.env` file to version control. Add it to your `.gitignore` file.

## Running the Application

1.  **Ensure your virtual environment is activated.**
2.  **Ensure your Ollama server is running (if using Ollama).**
3.  **Start the Streamlit application:**
    ```bash
    streamlit run app.py
    ```
    (Note: If you ran it in the background previously, you might need to kill the old process first: `kill $(ps aux | grep streamlit | grep -v grep | awk '{print $2}')`)

4.  Open your web browser and navigate to the URL provided by Streamlit (usually `http://localhost:8501`).

## Logging

All LLM calls are logged to `llm_calls.log` in JSON format. This file can be used for monitoring, debugging, and analysis of LLM interactions.

```json
{"message": "LLM call successful", "prompt_id": "sp-4f8c2e", "prompt_version": "0.1.0", "model_used": "llama2", "tokens_used": 123, "latency": 0.543, "outcome": "success", "time": "2025-10-23T10:30:00.123456"}
``````